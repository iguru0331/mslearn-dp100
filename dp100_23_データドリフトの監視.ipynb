{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a873d513",
   "metadata": {},
   "source": [
    "# dp100_23 データドリフトの監視\n",
    "\n",
    "トレーニングと推論の間にあるデータプロファイルのこの変化は*データドリフト*と呼ばれ、  \n",
    "運用環境で使用される予測モデルにとって重大な問題になる可能性がある。\n",
    "\n",
    "そのため、時間の経過に伴うデータドリフトを監視し、必要に応じてモデルを再トレーニングして予測精度を維持できるようにすることが重要。\n",
    "\n",
    "## データドリフトモニターの作成\n",
    "\n",
    "データセットを使用したデータドリフトの監視をサポートしており、  \n",
    "データセット内の新しい特徴量をキャプチャし、モデルのトレーニングに使用したデータセットと比較できる。\n",
    "\n",
    "### データセットを比較してデータドリフトを監視する\n",
    "\n",
    "モデルがトレーニングされた後も、組織が新しいデータを収集し続けることは一般的。  \n",
    "増え続ける新しいデータのコレクションを元のトレーニングデータと定期的に比較し、  \n",
    "モデルの精度に影響する可能性のあるデータの傾向の変化を特定することができる。\n",
    "\n",
    "登録済みのデータセットを使用してデータドリフトを監視するには、次の2つのデータセットを登録する必要がある。\n",
    "\n",
    "- *ベースライン\"データセット\n",
    "    - 通常は元のトレーニングデータ\n",
    "- 時間間隔に基づいてベースラインと比較される\"ターゲット\"データセット\n",
    "    - このデータセットには、比較する各特徴の列と、データドリフト率を測定するためのタイムスタンプ列が必要\n",
    "    \n",
    "> 注 : デプロイされたサービスを構成して、推論のためにモデルに送信された新しいデータを収集できる。  \n",
    "このデータはAzure Blob Storageに保存され、データドリフトの監視対象のデータセットとして使用できる。\n",
    "\n",
    "これらのデータセットを作成した後、*データセットモニター*を定義してデータドリフトを検出し、  \n",
    "指定されたしきい値を誤差率が越えた場合にアラートをトリガーできる。\n",
    "\n",
    "次のサンプルコードに示すように、AzureMLスタジオのビジュアルインターフェイスを使用するか、  \n",
    "SDKの**DataDriftDetector**クラスを使用してデータセットモニターを作成できる。\n",
    "\n",
    "```\n",
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "monitor = DataDriftDetector.create_from_datasets(workspace=ws,\n",
    "                                                 name='dataset-drift-detector',\n",
    "                                                 baseline_data_set=train_ds,\n",
    "                                                 target_data_set=new_data_ds,\n",
    "                                                 compute_target='aml-cluster',\n",
    "                                                 frequency='Week',\n",
    "                                                 feature_list=['age','height', 'bmi'],\n",
    "                                                 latency=24)\n",
    "```\n",
    "\n",
    "データセットモニターを作成した後、以下のように*backfill*を実行すると、  \n",
    "ベースラインデータセットをターゲットデータセットの既存のデータとすぐに比較できる。\n",
    "\n",
    "```\n",
    "import datetime as dt\n",
    "\n",
    "backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a7f50",
   "metadata": {},
   "source": [
    "## アラートのスケジュール設定\n",
    "\n",
    "データモニターを定義するときに、その実行スケジュールを指定する。  \n",
    "さらに、データドリフト率のしきい値と、このしきい値を超えた場合に通知するためのオペレータのメールアドレスを指定できる。\n",
    "\n",
    "### データドリフトモニターのスケジュールを構成する\n",
    "\n",
    "データドリフトの監視は、スケジュールが設定された**frequency**で比較を実行し、データドリフトメトリックを計算することで機能する。  \n",
    "**Day**、**Week**、または**Month**ごとに実行するスケジュールを定義できる。\n",
    "\n",
    "データセットモニターの場合、**latency**を指定して、新しいデータを収集し、ターゲットデータセットに追加できる時間数を示すことができる。\n",
    "デプロイされたモデルのデータドリフトモニターの場合、データドリフトの実行を開始するタイミングを示す**schedule_start**の時間値を指定できる。\n",
    "\n",
    "### アラートを構成する\n",
    "\n",
    "データドリフトを測定するには、特徴量の値の統計的分布の時間経過に伴う変化を計算した*magnitude*を使用する。  \n",
    "重大なデータドリフトを示す可能性がある大きな変化がないか監視する必要がある。\n",
    "\n",
    "通知を受けたいデータドリフトの規模の**しきい値**を定義し、メールに依るアラート通知を構成することができる。  \n",
    "以下のコードは、データドリフトモニターを毎週実行し、誤差の大きさが0.3を超える場合にアラートを送信するようにスケジュールを設定している。\n",
    "\n",
    "```\n",
    "alert_email = AlertConfiguration('data_scientists@contoso.com')\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'dataset-drift-detector', \n",
    "                                                 baseline_data_set, target_data_set,\n",
    "                                                 compute_target=cpu_cluster,\n",
    "                                                 frequency='Week', latency=2,\n",
    "                                                 drift_threshold=.3,\n",
    "                                                 alert_configuration=alert_email)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bea60f",
   "metadata": {},
   "source": [
    "## 演習　データドリフトの監視\n",
    "\n",
    "### 必要パッケージについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0060205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azureml-datadrift\n",
      "Version: 1.28.0\n",
      "Summary: Azure Machine Learning datadrift\n",
      "Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py\n",
      "Author: Microsoft Corp\n",
      "Author-email: None\n",
      "License: https://aka.ms/azureml-sdk-license\n",
      "Location: /anaconda/envs/azureml_py36/lib/python3.6/site-packages\n",
      "Requires: scipy, jsonpickle, scikit-learn, matplotlib, azureml-pipeline-core, azureml-core, azureml-dataset-runtime, azureml-telemetry, lightgbm, pandas, pyspark, msrest, numpy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show azureml-datadrift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3a8f5",
   "metadata": {},
   "source": [
    "### ワークスペースへの接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ac83d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to work with 20210613\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc9fe0",
   "metadata": {},
   "source": [
    "### ベースラインデータセットの作成\n",
    "\n",
    "データセットのデータドリフトを監視するためには、ベースラインデータセット(通常はモデルのトレーニングに使用したデータセット)を登録し、  \n",
    "将来収集されるデータとの比較のポイントとして使用する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39747a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading ./data/diabetes.csv\n",
      "Uploaded ./data/diabetes.csv, 1 files out of an estimated total of 2\n",
      "Uploading ./data/diabetes2.csv\n",
      "Uploaded ./data/diabetes2.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n",
      "Registering baseline dataset...\n",
      "Baseline dataset registered!\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "\n",
    "# Upload the baseline data\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'],\n",
    "                       target_path='diabetes-baseline',\n",
    "                       overwrite=True, \n",
    "                       show_progress=True)\n",
    "\n",
    "# Create and register the baseline dataset\n",
    "print('Registering baseline dataset...')\n",
    "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
    "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
    "                           name='diabetes baseline',\n",
    "                           description='diabetes baseline data',\n",
    "                           tags = {'format':'CSV'},\n",
    "                           create_new_version=True)\n",
    "\n",
    "print('Baseline dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe136296",
   "metadata": {},
   "source": [
    "### ターゲットデータセットの作成\n",
    "\n",
    "時間の経過とともに、ベースラインの訓練データセットと同じ特徴を持つ新しいデータを収集することができる。  \n",
    "この新しいデータをベースラインデータと比較するには、データドリフトを分析したい特徴量を含むターゲットデータセットを定義する必要がある。  \n",
    "タイムスタンプは、データセット自体のフィールド化、データの保存に使用されたフォルダとファイル名のパターンから得られるものがある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5774bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simulated data...\n",
      "Uploading an estimated of 6 files\n",
      "Uploading data/diabetes_2021-05-12.csv\n",
      "Uploaded data/diabetes_2021-05-12.csv, 1 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-05-19.csv\n",
      "Uploaded data/diabetes_2021-05-19.csv, 2 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-05-26.csv\n",
      "Uploaded data/diabetes_2021-05-26.csv, 3 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-06-02.csv\n",
      "Uploaded data/diabetes_2021-06-02.csv, 4 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-06-09.csv\n",
      "Uploaded data/diabetes_2021-06-09.csv, 5 files out of an estimated total of 6\n",
      "Uploading data/diabetes_2021-06-16.csv\n",
      "Uploaded data/diabetes_2021-06-16.csv, 6 files out of an estimated total of 6\n",
      "Uploaded 6 files\n",
      "Registering target dataset...\n",
      "Target dataset registered!\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "print('Generating simulated data...')\n",
    "\n",
    "# Load the smaller of the two data files\n",
    "data = pd.read_csv('data/diabetes2.csv')\n",
    "\n",
    "# We'll generate data for the past 6 weeks\n",
    "weeknos = reversed(range(6))\n",
    "\n",
    "file_paths = []\n",
    "for weekno in weeknos:\n",
    "    \n",
    "    # Get the date X weeks ago\n",
    "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
    "    \n",
    "    # Modify data to ceate some drift\n",
    "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
    "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
    "    data['BMI'] = data['BMI'] * 1.1\n",
    "    \n",
    "    # Save the file with the date encoded in the filename\n",
    "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
    "    data.to_csv(file_path)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Upload the files\n",
    "path_on_datastore = 'diabetes-target'\n",
    "default_ds.upload_files(files=file_paths,\n",
    "                       target_path=path_on_datastore,\n",
    "                       overwrite=True,\n",
    "                       show_progress=True)\n",
    "\n",
    "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
    "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
    "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
    "                                                       partition_format=partition_format)\n",
    "\n",
    "# Register the target dataset\n",
    "print('Registering target dataset...')\n",
    "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
    "                                                                          name='diabetes target',\n",
    "                                                                          description='diabetes target data',\n",
    "                                                                          tags = {'format':'CSV'},\n",
    "                                                                          create_new_version=True)\n",
    "\n",
    "print('Target dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f702a",
   "metadata": {},
   "source": [
    "### データドリフトモニターの作成\n",
    "\n",
    "データドリフトモニターは、定期的またはオンデマンドで実行され、ベースラインデータセットとターゲットデータセットを比較する。  \n",
    "※ターゲットデータセットには、時間の経過とともに新しいデータが追加される。\n",
    "\n",
    "#### コンピューティングターゲットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd85d949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"msl-20210613b\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f282880",
   "metadata": {},
   "source": [
    "#### データドリフトモニターの定義\n",
    "\n",
    "データドリフトを監視する特徴量、監視プロセスの実行に使用するコンピューティングターゲット名、データを比較する頻度、  \n",
    "アラートが発生するデータドリフトのしきい値、データ収集のための待ち時間を指定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f87cb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_workspace': Workspace.create(name='20210613', subscription_id='153404fd-72ab-4092-b50e-de490c5509fc', resource_group='20210613'), '_frequency': 'Week', '_schedule_start': None, '_schedule_id': None, '_interval': 1, '_state': 'Disabled', '_alert_config': None, '_type': 'DatasetBased', '_id': '6c10f0a1-8963-437c-95a7-cd7581c0ec57', '_model_name': None, '_model_version': 0, '_services': None, '_compute_target_name': 'msl-20210613b', '_drift_threshold': 0.3, '_baseline_dataset_id': 'e6f56144-90b4-4ee6-90c9-003825cfe4cd', '_target_dataset_id': '6dc1504e-756b-4cd2-81dd-527263e5a697', '_feature_list': ['Pregnancies', 'Age', 'BMI'], '_latency': 24, '_name': 'mslearn-diabates-drift', '_latest_run_time': None, '_client': <azureml.datadrift._restclient.datadrift_client.DataDriftClient object at 0x7f2d986b7860>, '_logger': <_TelemetryLoggerContextAdapter azureml.datadrift._logging._telemetry_logger.azureml.datadrift.datadriftdetector (DEBUG)>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "# set up feature list\n",
    "features = ['Pregnancies', 'Age', 'BMI']\n",
    "\n",
    "# set up data drift detector\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'mslearn-diabates-drift', baseline_data_set, target_data_set,\n",
    "                                                      compute_target=cluster_name, \n",
    "                                                      frequency='Week', \n",
    "                                                      feature_list=features, \n",
    "                                                      drift_threshold=.3, \n",
    "                                                      latency=24)\n",
    "monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbbae40",
   "metadata": {},
   "source": [
    "### データドリフトモニターのバックフィル(埋戻し)\n",
    "\n",
    "ベースラインデータセットとターゲットデータセットを使ってモニターをバックフィルし、オリジナルのベースラインデータとターゲットデータの間の  \n",
    "データドリフトを分析することができる。\n",
    "\n",
    "> 注 : バックフィル分析を行うためには、コンピューティングターゲットを起動する必要があるので、  \n",
    "実行に時間がかかる場合がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf2a173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41e9a8d850b46b598f9e241f4066fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/mslearn-diabates-drift-Monitor-Runs_1623863168511?wsid=/subscriptions/153404fd-72ab-4092-b50e-de490c5509fc/resourcegroups/20210613/workspaces/20210613&tid=5456e8d8-0223-4619-ba5b-e313627da53d\", \"run_id\": \"mslearn-diabates-drift-Monitor-Runs_1623863168511\", \"run_properties\": {\"run_id\": \"mslearn-diabates-drift-Monitor-Runs_1623863168511\", \"created_utc\": \"2021-06-16T17:06:10.901245Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"c9f8a4e3-334a-4a85-82a6-4316506b3599\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":0}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2021-06-16T17:21:23.481811Z\", \"status\": \"Completed\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=F0BzfbVdR6YwiMoGnJsBsQTX%2BSw%2FeqJQJgDurSLPoXg%3D&st=2021-06-17T00%3A13%3A53Z&se=2021-06-17T08%3A23%3A53Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt\": \"https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/55_azureml-execution-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt?sv=2019-02-02&sr=b&sig=byZSO2Caf9mQlQEZ%2ByDHYJWdxKPyPRlkJxZachWAsGk%3D&st=2021-06-17T00%3A13%3A53Z&se=2021-06-17T08%3A23%3A53Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt\": \"https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/65_job_prep-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt?sv=2019-02-02&sr=b&sig=Bh1WtXI2IzT%2FOLjaaY89ZIgLHU2vikZWHPvXT69%2FAdg%3D&st=2021-06-17T00%3A13%3A53Z&se=2021-06-17T08%3A23%3A53Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=9Zhgnh5qriLZXoAGKx3z15o8dWqZdXXinapyIO9brqo%3D&st=2021-06-17T00%3A13%3A53Z&se=2021-06-17T08%3A23%3A53Z&sp=r\", \"azureml-logs/75_job_post-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt\": \"https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/75_job_post-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt?sv=2019-02-02&sr=b&sig=QHRzorTySzNRM0VdxT6nbeDRVin4Cd%2FM1psTw1DGu70%3D&st=2021-06-17T00%3A13%3A53Z&se=2021-06-17T08%3A23%3A53Z&sp=r\", \"azureml-logs/process_info.json\": \"https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=EmBlLNs4q8VENacMKwFIlGqkDew6G2WuLG5b8DKA5aQ%3D&st=2021-06-17T00%3A13%3A53Z&se=2021-06-17T08%3A23%3A53Z&sp=r\", \"azureml-logs/process_status.json\": \"https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=eU1iw0IBJ97kytpi22BXOiWr08nLUBcqOAaz0jrirdI%3D&st=2021-06-17T00%3A13%3A53Z&se=2021-06-17T08%3A23%3A53Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\"], [\"azureml-logs/20_image_build_log.txt\"], [\"azureml-logs/55_azureml-execution-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt\"]], \"run_duration\": \"0:15:12\", \"run_number\": \"1\", \"run_queued_details\": {\"status\": \"Completed\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"start_date\", \"run_id\": \"mslearn-diabates-drift-Monitor-Runs_1623863168511\", \"categories\": [0], \"series\": [{\"data\": [\"2021-05-02\"]}]}, {\"name\": \"end_date\", \"run_id\": \"mslearn-diabates-drift-Monitor-Runs_1623863168511\", \"categories\": [0], \"series\": [{\"data\": [\"2021-06-20\"]}]}, {\"name\": \"frequency\", \"run_id\": \"mslearn-diabates-drift-Monitor-Runs_1623863168511\", \"categories\": [0], \"series\": [{\"data\": [\"Week\"]}]}, {\"name\": \"Datadrift percentage\", \"run_id\": \"mslearn-diabates-drift-Monitor-Runs_1623863168511\", \"categories\": [0], \"series\": [{\"data\": [{\"days_from_start\": [7, 14, 21, 28, 35, 42], \"drift_percentage\": [74.19152901127207, 87.23985219136877, 91.74192122865539, 94.96492628559955, 97.58354951107833, 99.23199438682525]}]}]}], \"run_logs\": \"[2021-06-16T17:21:10.873309] Entering job release\\r\\n[2021-06-16T17:21:11.655365] Starting job release\\r\\n[2021-06-16T17:21:11.655939] Logging experiment finalizing status in history service.\\r\\nStarting the daemon thread to refresh tokens in background for process with pid = 1107\\r\\n[2021-06-16T17:21:11.656298] job release stage : upload_datastore starting...\\r\\n[2021-06-16T17:21:11.662976] job release stage : start importing azureml.history._tracking in run_history_release.\\r\\n[2021-06-16T17:21:11.663005] job release stage : execute_job_release starting...\\r\\n[2021-06-16T17:21:11.663338] job release stage : copy_batchai_cached_logs starting...\\r\\n[2021-06-16T17:21:11.663465] job release stage : copy_batchai_cached_logs completed...\\r\\n[2021-06-16T17:21:11.663917] Entering context manager injector.\\r\\n[2021-06-16T17:21:11.665103] job release stage : upload_datastore completed...\\r\\n[2021-06-16T17:21:11.841022] job release stage : send_run_telemetry starting...\\r\\n[2021-06-16T17:21:11.955727] job release stage : execute_job_release completed...\\r\\n[2021-06-16T17:21:12.232417] get vm size and vm region successfully.\\r\\n[2021-06-16T17:21:12.602869] get compute meta data successfully.\\r\\n[2021-06-16T17:21:12.837991] post artifact meta request successfully.\\r\\n[2021-06-16T17:21:12.895988] upload compute record artifact successfully.\\r\\n[2021-06-16T17:21:12.896077] job release stage : send_run_telemetry completed...\\r\\n[2021-06-16T17:21:12.896551] Job release is complete\\r\\n\\nRun is completed.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.28.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'mslearn-diabates-drift-Monitor-Runs_1623863168511',\n",
       " 'target': 'msl-20210613b',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2021-06-16T17:16:11.266838Z',\n",
       " 'endTimeUtc': '2021-06-16T17:21:23.481811Z',\n",
       " 'warnings': [{'source': 'datadrift',\n",
       "   'message': 'target dataset id:6dc1504e-756b-4cd2-81dd-527263e5a697 do not contain sufficient amount of data after timestamp filteringMinimum needed: 50 rows.Skipping calculation for time slice 2021-05-02 00:00:00 to 2021-05-09 00:00:00.'}],\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': 'c9f8a4e3-334a-4a85-82a6-4316506b3599',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': 'e6f56144-90b4-4ee6-90c9-003825cfe4cd'}, 'consumptionDetails': {'type': 'Reference'}}, {'dataset': {'id': '6dc1504e-756b-4cd2-81dd-527263e5a697'}, 'consumptionDetails': {'type': 'Reference'}}],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'script': '_generate_script_datasets.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--baseline_dataset_id',\n",
       "   'e6f56144-90b4-4ee6-90c9-003825cfe4cd',\n",
       "   '--target_dataset_id',\n",
       "   '6dc1504e-756b-4cd2-81dd-527263e5a697',\n",
       "   '--workspace_name',\n",
       "   '20210613',\n",
       "   '--workspace_location',\n",
       "   'westus2',\n",
       "   '--instrumentation_key',\n",
       "   'baa85199-67b9-4e16-b267-4325f944fef7',\n",
       "   '--ai_endpoint',\n",
       "   'https://dc.applicationinsights.azure.com/v2/track',\n",
       "   '--subscription_id',\n",
       "   '153404fd-72ab-4092-b50e-de490c5509fc',\n",
       "   '--enable_metric_logger',\n",
       "   'true',\n",
       "   '--run_type',\n",
       "   'BackFill',\n",
       "   '--drift_threshold',\n",
       "   '0',\n",
       "   '--datadrift_id',\n",
       "   '6c10f0a1-8963-437c-95a7-cd7581c0ec57',\n",
       "   '--datadrift_run_id',\n",
       "   '1330a0f0-e6fa-4116-b374-93a7e191ba06',\n",
       "   '--datadrift_name',\n",
       "   'mslearn-diabates-drift',\n",
       "   '--frequency',\n",
       "   'Week',\n",
       "   '--datadrift_configuration_type',\n",
       "   'DatasetBased',\n",
       "   '--start_date',\n",
       "   '2021-05-02',\n",
       "   '--end_date',\n",
       "   '2021-06-20',\n",
       "   '--features_whitelist',\n",
       "   'Pregnancies',\n",
       "   'Age',\n",
       "   'BMI'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'msl-20210613b',\n",
       "  'dataReferences': {},\n",
       "  'data': {},\n",
       "  'outputData': {},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'Experiment mslearn-diabates-drift-Monitor-Runs Environment',\n",
       "   'version': 'Autosave_2021-06-16T17:06:08Z_4f749cd4',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'dependencies': ['python=3.6.2',\n",
       "      'scikit-learn',\n",
       "      'scipy>=1.0.0',\n",
       "      'numpy',\n",
       "      'lightgbm<=3.1.0',\n",
       "      'pandas',\n",
       "      'pyarrow>=0.11.0',\n",
       "      'jsonpickle',\n",
       "      'psutil',\n",
       "      {'pip': ['azureml-defaults==1.28.0', 'azureml-datadrift==1.28.0']}],\n",
       "     'name': 'azureml_3bb84aab3adfd5098f0c8cb5b96ad428'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None,\n",
       "     'username': None,\n",
       "     'password': None}},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': None,\n",
       "   'enableMLflowTracking': False},\n",
       "  'spark': {'configuration': {}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'aiSuperComputer': {'instanceType': None,\n",
       "   'imageVersion': None,\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None},\n",
       "  'tensorflow': {'workerCount': 0, 'parameterServerCount': 0},\n",
       "  'mpi': {'processCountPerNode': 0},\n",
       "  'pyTorch': {'communicationBackend': None, 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'None'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {}},\n",
       " 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=YWFERf%2BV7wO72LMpyGMcSpmLbuYVh2Y6g12PcEp3pJU%3D&st=2021-06-16T17%3A11%3A31Z&se=2021-06-17T01%3A21%3A31Z&sp=r',\n",
       "  'azureml-logs/55_azureml-execution-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt': 'https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/55_azureml-execution-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt?sv=2019-02-02&sr=b&sig=DqTkOCMWgssX45l6qSbUO5z1G%2Bfq265dD3ExeEFZSsw%3D&st=2021-06-16T17%3A11%3A31Z&se=2021-06-17T01%3A21%3A31Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt': 'https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/65_job_prep-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt?sv=2019-02-02&sr=b&sig=kabGiHDHDnME30tJFgw60oL2DsT3CF77hgpyOHUwdn8%3D&st=2021-06-16T17%3A11%3A31Z&se=2021-06-17T01%3A21%3A31Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=%2Br5SvTpU9Ft6vyVFD%2FMIFTRKX7MIiemvpmbwRpFzPUU%3D&st=2021-06-16T17%3A11%3A31Z&se=2021-06-17T01%3A21%3A31Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt': 'https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/75_job_post-tvmps_e89cfd5a7584d6588e6ed908e8918ee8b340d70deee360d514b46e8bd4b1a7f6_d.txt?sv=2019-02-02&sr=b&sig=mfbCqW5WdhBce5jL%2Bu%2BhywUk3Daet3vsuKKDCm9Y4EU%3D&st=2021-06-16T17%3A11%3A31Z&se=2021-06-17T01%3A21%3A31Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=7OVsgjceMdD7Z0LtaHrxCTblCt55RUeB9CupwdCwj6Y%3D&st=2021-06-16T17%3A11%3A31Z&se=2021-06-17T01%3A21%3A31Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://202106138491592323.blob.core.windows.net/azureml/ExperimentRun/dcid.mslearn-diabates-drift-Monitor-Runs_1623863168511/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=m060gwwfwX6jLQ9wTnip11G3M272FsSEZZd0zBR38K8%3D&st=2021-06-16T17%3A11%3A31Z&se=2021-06-17T01%3A21%3A31Z&sp=r'},\n",
       " 'submittedBy': 'Tatsuya Kato'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "\n",
    "RunDetails(backfill).show()\n",
    "backfill.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39560d4b",
   "metadata": {},
   "source": [
    "### データドリフトの解析\n",
    "\n",
    "以下のコードを使用して、バックフィル実行で収集されたポイントインタイムのデータドリフトを調べることもできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ed607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_date 2021-05-02\n",
      "end_date 2021-06-20\n",
      "frequency Week\n",
      "Datadrift percentage {'days_from_start': [7, 14, 21, 28, 35, 42], 'drift_percentage': [74.19152901127207, 87.23985219136877, 91.74192122865539, 94.96492628559955, 97.58354951107833, 99.23199438682525]}\n"
     ]
    }
   ],
   "source": [
    "drift_metrics = backfill.get_metrics()\n",
    "for metric in drift_metrics:\n",
    "    print(metric, drift_metrics[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e92518",
   "metadata": {},
   "source": [
    "AzureMLスタジオでデータドリフトメトリックを可視化することもできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d985d",
   "metadata": {},
   "source": [
    "> データドリフトの監視の詳細URL : https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets  \n",
    "> 公開されているサービスからデータを収集して、データドリフト監視の対象データセットとして使う場合のURL : https://docs.microsoft.com/azure/machine-learning/how-to-enable-data-collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699d367",
   "metadata": {},
   "source": [
    "## 知識チェック\n",
    "\n",
    "1. 昨年収集されたデータを含むデータセットを使用してモデルをトレーニングしました。  \n",
    "今年は、新しいデータを収集する予定です。 モデルのパフォーマンスに影響する可能性のある、変化するデータの傾向を追跡したいと考えています。  \n",
    "何をする必要がありますか?\n",
    "\n",
    "    - 既存のトレーニング データセットの新しいバージョンで新しいデータを収集し、両方のデータセットをプロファイルします。\n",
    "    - 別のデータセットで新しいデータを収集し、トレーニング データセットをベースラインとして、  \n",
    "    新しいデータセットをターゲットとして使用するデータ ドリフト モニターを作成します。\n",
    "    - トレーニング データセットを、元のトレーニング データと新しいデータの両方を含む新しいデータセットに置き換えます。\n",
    "\n",
    "\n",
    "2. あなたはデータ ドリフト モニターを作成しています。 データ分布に大きな変化が検出された場合、  \n",
    "データ サイエンス チームに自動的に通知したいと考えています。 どうすればよいでしょうか。\n",
    "\n",
    "    - AlertConfiguration を定義し、drift_threshold 値を設定します。\n",
    "    - データ サイエンティストが新しいデータを確認する時間を確保できるように、データ ドリフト モニターの待機時間を設定します。\n",
    "    - データ サイエンス チームのメール アドレスをタグとして含め、トレーニング データセットをモデルに登録します。\n",
    "\n",
    "↓解答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de0786",
   "metadata": {},
   "source": [
    "1. 別のデータセットで新しいデータを収集し、トレーニング データセットをベースラインとして、  \n",
    "新しいデータセットをターゲットとして使用するデータ ドリフト モニターを作成します。\n",
    "\n",
    "    - 変化するデータの傾向を追跡するために、トレーニング データをベースラインとして、  \n",
    "    新しいデータをターゲットとして使用するデータ ドリフト モニターを作成します。\n",
    "\n",
    "\n",
    "2. AlertConfiguration を定義し、drift_threshold 値を設定します。\n",
    "\n",
    "    - データ ドリフトについてオペレーターに通知するには、通知先のメール アドレスと、  \n",
    "    通知をトリガーする変化のレベルを定義する誤差のしきい値を指定した AlertConfiguration を作成します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
